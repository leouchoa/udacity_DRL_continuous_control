% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Double-jointed Arm Reacher Project Solution},
  pdfauthor={Leonardo Uchoa Pedreira},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering

\title{Double-jointed Arm Reacher Project Solution}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Udacity Deep Reinforcement Learning Project Number 02}
\author{Leonardo Uchoa Pedreira}
\date{7/28/2021}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\hypertarget{description}{%
\section{Description}\label{description}}

This document is a report describing the learning algorithm and details
of implementation, along with ideas for future work.

\pagebreak

\hypertarget{algorithm-ddpg}{%
\section{Algorithm: DDPG}\label{algorithm-ddpg}}

Deep Deterministic Policy Gradients, or DDPG for short, is an
Actor-Critic Reinforcement Learning method
\href{https://arxiv.org/pdf/1509.02971.pdf}{created} to enable agents to
better learn optimal policies to behave in continuous action spaces.
DDPG borrows ideas from both \texttt{DQN} and \texttt{DPG} (DPG stands
for Deterministic Policy Gradients).

The \texttt{DQN} is a method where a neural network is used is to
implement the \texttt{Q-Learning} algorithm, which attempts to estimate
action-value pairs in order to maximize the expected total reward and,
therefore, to obtain the optimal policy for the given task. It belongs
to class of value-based methods, whose goal is to solve the
\href{https://en.wikipedia.org/wiki/Bellman_equation}{bellman equation}.
Solving the bellman equation gives us the optimal policy, given that our
environment meets certain criteria in our Markov Decision Process
setting. For \texttt{Q-Learning} in particular the equation we're trying
to solve is as follows

\[
\displaystyle Q^{new}(s_{t},a_{t})\leftarrow \underbrace {Q(s_{t},a_{t})} _{\text{old value}}+\underbrace {\alpha } _{\text{learning rate}}\cdot \overbrace {{\bigg (}\underbrace {\underbrace {r_{t}} _{\text{reward}}+\underbrace {\gamma } _{\text{discount factor}}\cdot \underbrace {\max _{a}Q(s_{t+1},a)} _{\text{estimate of optimal future value}}} _{\text{new value (temporal difference target)}}-\underbrace {Q(s_{t},a_{t})} _{\text{old value}}{\bigg )}} ^{\text{temporal difference}} 
\]

So in a given time step \texttt{t} we search for the action that
maximizes the action-value pair \(Q(s_{t+1},a)\) in order to estimate
the optimal future value. DQN uses neural networks to step up from the
traditional tabular method approach to the function approximation
approach, which means that instead of storing the q-values in a table,
we're going to encode it in a \textbf{parametrized} function
approximator (parametrization what turn the q-value from \(q(s,a)\) to
\(q(s,a;\theta)\)). This gives us a lot flexibility to solve many other
problems, specially those whose state space is continuous.

The figure bellow is the \texttt{DQN} implementation proposed in the
\href{https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf}{nature
paper}. In that implementation there are two important additional
tricks. Those tricks are employed in order to solve some instability
that the neural network suffers in training. The first is experience
replay and the second is the fixed q-targets.

Experience replay is mainly just creating a buffer to store some events
(the \((S_{t},A_{t},R_{t},S_{t+1})\) action-pairs) and then latter using
mini-batches of them to run gradient descent and learn the network
weights.

Fixed q-targets is a technique used to avoid a difficulty found in
Q-Learning, where we update a guess with a guess, which can potentially
lead to harmful correlations. In Q-Learning after each pass our neural
network tries to get as close as possible to the target q-values, but
the problem is that because we update the weights after each forward
pass, our q-value target to calculate the loss function is always
changing and that makes learning a bit unstable. To solve this problem
the idea is to store the weights in another neural network \(\hat{q}\)
(called target network), freeze \(\hat{q}\) the weights and update them
every once in a while. Now when performing the learning step, our
q-value loss function is the deviation from the forward pass calculated
\(q\) and the target (fixed) \(\hat{q}\) value. That means that out
``ground-truth'' response variable is the fixed q-value from the target
network and that makes learning more stable.

\begin{figure}
\centering
\includegraphics{imgs/dqn_print_1.png}
\caption{DQN paper implementation}
\end{figure}

The Deterministic Policy Gradients, or \texttt{DPG}, is an actor-critic
method that attempts to combine both value-based and policy-based
methods to learn an optimal policy IN continuous action spaces.
Following the original paper,
\href{http://proceedings.mlr.press/v32/silver14.pdf}{Deterministic
Policy Gradient Algorithms}, on the \texttt{DPG}:

\begin{quote}
The deterministic policy gradient has a particularly appealing form: it
is the expected gradient of the action-value function. This simple form
means that the deterministic policy gradient can be estimated much more
efficiently than the usual stochastic policy gradient.
\end{quote}

The \texttt{DDPG} algorithm then is a mix of both \texttt{DPG} and
\texttt{DQN}, where we have two networks: the actor and critic. The
actor is responsible for estimating the value (or action-value) function
while the critic is responsible for evaluating the value function. So
the intuition behind actor-critic methods goes like this:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  the actor takes in the current state and takes an action, based on the
  current policy \(\pi(a|s;\theta_{\pi})\).
\item
  collect the experience tuple \((s,a,r,s')\) to feed the critic, whose
  job is to estimate \(V(s;\theta_{v})\).
\item
  use the critic output to calculate the advantage function
  \(A(s,a)=r + \gamma V(s';\theta_{v}) - V(s;\theta_{v})\) and use it as
  the new baseline for actor to move on.
\item
  Repeat steps 1 through 3 until a stopping rule is achieved (end
  learning).
\end{enumerate}

But the \texttt{DDPG} algorithm is a bit different from classic
actor-critic methods. The actor in \texttt{DDPG} is used to approximate
\(\max _{a}Q(s_{t+1},a)\), an expression found the original \texttt{DQN}
implementation and not as a learn baseline. So instead of outputting
\(\pi(a|s;\theta_{\pi})\), the actor outputs \(\mu(s;\theta_{\mu})\), an
approximation of \(\max _{a}Q(s_{t+1},a)\) (notice that this is follows
an deterministic policy, not a stochastic policy). Then the critic takes
the actor output and learns to evaluate it, by estimating
\(Q(s,\mu(s;\theta_{\mu});\theta_{Q})\).

Besides that, \texttt{DDPG} is very much like what was described in
steps 1-4. So in the \texttt{DDPG} algorithm implementation of steps 1
through 4 the actor will have two neural networks (a target and a policy
network, like in the \texttt{DQN}) and for the critic we also have two
neural networks (a target and a policy network, also like in the
\texttt{DQN}). Also present in the \texttt{DDPG} algorithm is the replay
buffer, in order improve learning by collecting some chunks of episodes.
Also present are the fixed q targets used to help stabilize learning, as
described in the \texttt{DQN} implementation. \texttt{DDPG} pretty much
resembles the \texttt{DQN} algorithm, but addapted to support continuous
action spaces. The \texttt{DDPG} is depicted in Figure 02.

\begin{figure}
\centering
\includegraphics{imgs/ddpg_print_1.png}
\caption{DDPG paper implementation}
\end{figure}

\pagebreak

\hypertarget{coding-the-algorithm}{%
\section{Coding the Algorithm}\label{coding-the-algorithm}}

The code implementation of the algorithm used here is mostly inspired
the by
``\href{https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum}{Pendulum}''
project notebook provided by \texttt{Udacity}, in the ``Actor-Critic
Methods'' section. Here is a brief description of the files:

\begin{itemize}
\item
  \textbf{Reacher.ipynb}: a jupyter notebook that serves as a wrapper of
  smaller functions. This is the main file, responsible for: 1) starting
  the environment; 2) loading the neural network architecture in
  \texttt{model\_second\_env.py}; 3) loading the \texttt{ddpg\_agent}
  implementation, running it and saving model weights and score results.
\item
  \textbf{model\_second\_env.py}: this is where the neural network
  architectures are stored, written in \texttt{pytorch}.
\item
  \textbf{ddpg\_agent\_second\_env.py}: this is code that implements the
  \texttt{DDPG} high-level ideas of fixed q-targets, experience replay
  and Ornstein-Uhlenbeck noise.
\item
  \textbf{checkpoint\_actor\_second\_env.pth}: this is the \texttt{DDPG}
  actor model weights used to solve the environment.
\item
  \textbf{checkpoint\_critic\_second\_env.pth}: this is the
  \texttt{DDPG} critic model weights used to solve the environment.
\item
  \textbf{second\_env\_model\_results.sav}: this a pickled object that
  contains the scores, average scores and time spent training.
\end{itemize}

\hypertarget{comments-on-model_second_env.py}{%
\subsection{\texorpdfstring{Comments on
\texttt{model\_second\_env.py}}{Comments on model\_second\_env.py}}\label{comments-on-model_second_env.py}}

The \texttt{model\_second\_env.py} file contains the \texttt{pytorch}
model architecture used to solve the environment. The input size that
the network expects is the 33 variables corresponding to position,
rotation, velocity, and angular velocities of the arm, while the output
layer is a simple linear layer with output size of 4 corresponding to
torque applicable to two joints.

The \textbf{Actor Neural Networks} use the following architecture.

\begin{verbatim}
Input nodes (33) 
  -> Fully Connected Layer (128 nodes, Relu activation) 
    -> Batch Normlization
      -> Fully Connected Layer (128 nodes, Relu activation) 
         -> Ouput nodes (4 nodes, tanh activation)
\end{verbatim}

The \textbf{Critic Neural Networks} use the following architecture :

\begin{verbatim}
Input nodes (33) 
  -> Fully Connected Layer (128 nodes, Relu activation) 
    -> Batch Normlization
      -> Include Actions at the second fully connected layer
        -> Fully Connected Layer (128+4 nodes, Relu activation) 
          -> Ouput node (1 node, no activation)
\end{verbatim}

The \texttt{hyperparameters} used are:

\begin{itemize}
\tightlist
\item
  the number of units in both first and second fully connected layer are
  128.
\item
  the learning rate parameter used in the \texttt{adam} optimizer for
  both actor and critic are \texttt{LR\_ACTOR\ =\ LR\_CRITIC\ =\ 1e-4}
  (this actually belongs to the \texttt{ddpg\_agent\_second\_env.py}
  file, but it's also part of the neural network architecture)
\end{itemize}

\hypertarget{comments-on-ddpg_agent_second_env.py}{%
\subsection{\texorpdfstring{Comments on
\texttt{ddpg\_agent\_second\_env.py}}{Comments on ddpg\_agent\_second\_env.py}}\label{comments-on-ddpg_agent_second_env.py}}

The \texttt{ddpg\_agent\_second\_env.py} file contains:

\begin{itemize}
\tightlist
\item
  the \texttt{hyperparameters} used:

  \begin{itemize}
  \tightlist
  \item
    \texttt{BATCH\_SIZE\ =\ 128}: neural network mini-batch size
  \item
    \texttt{GAMMA\ =\ 0.99}: the discount factor used in the discounted
    sum of rewards
  \item
    \texttt{TAU\ =\ 1e-3}: the \(\tau\) parameter used to soft update of
    target parameters
  \item
    \texttt{LR\_ACTOR\ =\ 1e-4}: the actor neural network learning rate
    use in gradient descent
  \item
    \texttt{LR\_CRITIC\ =\ 1e-4}: the critic neural network learning
    rate use in gradient descent
  \item
    \texttt{WEIGHT\_DECAY\ =\ 0.0}: L2 weight decay
  \item
    \texttt{BUFFER\_SIZE\ =\ 1e6}: replay buffer size (how much we store
    into the replay buffer)
  \end{itemize}
\item
  the \texttt{Agent} class:

  \begin{itemize}
  \tightlist
  \item
    implements the \texttt{step}, \texttt{act}, \texttt{learn} and
    \texttt{soft\_updates} methods. Also has the \texttt{start\_learn}
    to implement the idea to learn just after some episodes in order to
    help stabilize training (as suggested in the benchmark section)
  \item
    initializes the policy and target networks for fixed q-target
    strategy for both actor and critic networks.
  \item
    initializes the \texttt{ReplayBuffer} class.
  \end{itemize}
\item
  the \texttt{OUNoise} class:

  \begin{itemize}
  \tightlist
  \item
    implements the Ornstein-Uhlenbeck process in order to help
    exploration
  \item
    hyperparameters used are the default already present in the udacity
    pendulum implementation. Namely:

    \begin{itemize}
    \tightlist
    \item
      \texttt{mu\ =\ 0}
    \item
      \texttt{theta\ =\ 0.15}
    \item
      \texttt{sigma\ =\ 0.2}
    \end{itemize}
  \item
    changed only to adapt the code the multiple agents environment
  \end{itemize}
\item
  the \texttt{ReplayBuffer} class: an API to

  \begin{itemize}
  \tightlist
  \item
    initialize storage
  \item
    implement the \texttt{add} (add new experiences) and \texttt{sample}
    (sample experiences to mini-batch GD learning) methods
  \end{itemize}
\end{itemize}

\pagebreak

\hypertarget{results}{%
\section{Results}\label{results}}

The \texttt{DDPG} implementation was able to successfully solve the task
of achieving an average score of +30 over 100 episodes in 144 episodes.
Its average score was 30.01, as shown in the \texttt{Figure\ 03} bellow.
It's important to note that we used a early stopping technique not to go
into the 2000 episodes, if not needed. The plot of the scores is shown
in `Figure 04.

\begin{figure}
\centering
\includegraphics{imgs/results_1.png}
\caption{Learning evolution}
\end{figure}

\begin{figure}
\centering
\includegraphics{imgs/results_2.png}
\caption{Learning evolution plot}
\end{figure}

\pagebreak

\hypertarget{ideas-to-explore-later}{%
\section{Ideas to Explore Later}\label{ideas-to-explore-later}}

Some ideas that could lead to improvements in the learning process are
trying to:

\begin{itemize}
\item
  as suggested in the benchmark subsection of the continuous control
  project section, something to try latter is to implement the TRPO,
  TNPG and also PPO algorithms, they may yield better results
\item
  In the actor-critic subsection some other algorithms are shown:
  A2C,A3C,GAE.
\item
  i would really like to try
  \href{https://openreview.net/pdf?id=SyZipzbCb}{D4PG}!
\item
  also besides trying very hard I found that solving the environment
  with just one actor was harder than with two. I'd like to try
  implementing the start\_learn function in the single environment to
  see it works as well as in the second one.
\item
  also the Ornstein-Uhlenbeck process to introduce noise in the
  parameters was an intersting idead. I'd like to explore more processes
  to see how they can change the agent learning, when everything else is
  ``fixed''.
\end{itemize}

\end{document}
